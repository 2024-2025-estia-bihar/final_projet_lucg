{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../data/image_classification/split_train.csv\")\n",
    "val_df = pd.read_csv(\"../data/image_classification/split_val.csv\")\n",
    "test_df = pd.read_csv(\"../data/image_classification/split_test.csv\")\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "# Load the text and image model\n",
    "text_model = load_model(\"path_to_text_model.h5\")\n",
    "image_model = load_model(\"path_to_image_model.h5\")\n",
    "\n",
    "print(\"Text model and image model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Function to load images\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize for model input\n",
    "    image = image / 255.0  # normalize\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_labels(labels):\n",
    "    return tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_array = np.array(train_df.iloc[:, 3:].values)\n",
    "\n",
    "class_counts = labels_array.sum(axis=0)  # Count samples per class\n",
    "class_weights = {\n",
    "    i: len(labels_array) / (len(class_counts) * class_counts[i])\n",
    "    for i in range(len(class_counts))\n",
    "}\n",
    "\n",
    "\n",
    "def compute_sample_weights(labels, class_weights):\n",
    "    return np.array(\n",
    "        [\n",
    "            sum(class_weights[i] * label[i] for i in range(len(label)))\n",
    "            for label in labels\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "sample_weights = compute_sample_weights(labels_array, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # normalisation\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # suppression des stop words english\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = pd.read_csv(\"../data/image_classification/train.csv\", on_bad_lines=\"skip\")\n",
    "data[\"Processed_Caption\"] = data[\"Caption\"].apply(preprocess_text)\n",
    "\n",
    "train_df[\"Processed_Caption\"] = train_df[\"Caption\"].apply(preprocess_text)\n",
    "val_df[\"Processed_Caption\"] = val_df[\"Caption\"].apply(preprocess_text)\n",
    "test_df[\"Processed_Caption\"] = test_df[\"Caption\"].apply(preprocess_text)\n",
    "\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"Processed_Caption\"])\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Vocabulaire: {len(word_index)} mots uniques\")\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    return pad_sequences(sequences, maxlen=max_sequence_length, padding=\"post\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = (\n",
    "    tf.data.Dataset.zip(\n",
    "        (\n",
    "            tf.data.Dataset.from_tensor_slices(train_df[\"Processed_Caption\"]).map(\n",
    "                tokenize_text\n",
    "            ),\n",
    "            tf.data.Dataset.from_tensor_slices(train_df[\"ImageURL\"]).map(load_image),\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                process_labels(train_df.iloc[:, 3:].values)\n",
    "            ),\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                sample_weights\n",
    "            ),  # Add sample weights to dataset\n",
    "        )\n",
    "    )\n",
    "    .batch(32)\n",
    "    .prefetch(1)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.zip(\n",
    "        (\n",
    "            tf.data.Dataset.from_tensor_slices(val_df[\"Processed_Caption\"]).map(\n",
    "                tokenize_text\n",
    "            ),\n",
    "            tf.data.Dataset.from_tensor_slices(val_df[\"ImageURL\"]).map(load_image),\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                process_labels(val_df.iloc[:, 3:].values)\n",
    "            ),  # Assuming labels are in columns 1 to n\n",
    "        )\n",
    "    )\n",
    "    .batch(32)\n",
    "    .prefetch(1)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.zip(\n",
    "        (\n",
    "            tf.data.Dataset.from_tensor_slices(test_df[\"Processed_Caption\"]).map(\n",
    "                tokenize_text\n",
    "            ),\n",
    "            tf.data.Dataset.from_tensor_slices(test_df[\"ImageURL\"]).map(load_image),\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                process_labels(test_df.iloc[:, 3:].values)\n",
    "            ),  # Assuming labels are in columns 1 to n\n",
    "        )\n",
    "    )\n",
    "    .batch(32)\n",
    "    .prefetch(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des caractéristiques textuelles\n",
    "text_features = Model(\n",
    "    inputs=text_model.input, outputs=text_model.get_layer(index=-4).output\n",
    ")\n",
    "\n",
    "# Extraction des caractéristiques visuelles\n",
    "visual_features = Model(\n",
    "    inputs=image_model.input, outputs=image_model.get_layer(index=-4).output\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Caractéristiques textuelles :\", text_features)\n",
    "print(\"Caractéristiques visuelles :\", visual_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les caractéristiques textuelles et visuelles\n",
    "def concatenate_features(text_features, visual_features):\n",
    "    return tf.concat([text_features, visual_features], axis=-1)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Supposons que `text_features_output` et `visual_features_output` soient les sorties des modèles respectifs\n",
    "text_features_output = text_features.predict(train_ds.map(lambda x, y, z, w: x))\n",
    "visual_features_output = visual_features.predict(train_ds.map(lambda x, y, z, w: y))\n",
    "\n",
    "common_representation = concatenate_features(\n",
    "    text_features_output, visual_features_output\n",
    ")\n",
    "print(\"Représentation commune des données créée : \", common_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# Ajouter des couches denses pour la classification multi-label\n",
    "def build_classification_head(input_features, num_classes):\n",
    "    x = Dense(256, activation=\"relu\")(input_features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "num_classes = train_df.iloc[:, 3:].shape[\n",
    "    1\n",
    "]  # Nombre de classes basé sur les colonnes des étiquettes\n",
    "classification_output = build_classification_head(common_representation, num_classes)\n",
    "\n",
    "print(\"Classification head ajoutée avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement conjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Define inputs for text and image\n",
    "text_input = Input(shape=(max_sequence_length,), name=\"text_input\")\n",
    "image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "\n",
    "# Extract features using pre-trained models\n",
    "text_features_output = text_features(text_input)\n",
    "visual_features_output = visual_features(image_input)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = concatenate_features(text_features_output, visual_features_output)\n",
    "\n",
    "# Add classification head\n",
    "classification_output = build_classification_head(combined_features, num_classes)\n",
    "\n",
    "# Define the final model\n",
    "combined_model = Model(inputs=[text_input, image_input], outputs=classification_output)\n",
    "\n",
    "# Compile the model\n",
    "combined_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.AUC(\n",
    "            multi_label=True,\n",
    "            num_thresholds=200,\n",
    "            curve=\"ROC\",\n",
    "            summation_method=\"interpolation\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "    ),\n",
    "    # tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = combined_model.fit(\n",
    "    train_ds.map(lambda x, y, z, w: ((x, y), z), num_parallel_calls=AUTOTUNE),\n",
    "    validation_data=val_ds.map(\n",
    "        lambda x, y, z: ((x, y), z), num_parallel_calls=AUTOTUNE\n",
    "    ),\n",
    "    epochs=5,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"Evaluation du modèle sur le jeu de validation :\")\n",
    "combined_model.evaluate(\n",
    "    val_ds.map(lambda x, y, z: ((x, y), z), num_parallel_calls=AUTOTUNE), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_threshold(y_pred, threshold=0.5, threshold_nb=None):\n",
    "    if threshold_nb is not None:\n",
    "        y_pred_top3 = np.zeros_like(y_pred, dtype=int)\n",
    "        for i in range(len(y_pred)):\n",
    "            top_indices = np.argsort(y_pred[i])[-threshold_nb:]\n",
    "            y_pred_top3[i, top_indices] = 1\n",
    "\n",
    "        return y_pred_top3\n",
    "\n",
    "    return (y_pred > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple avec un échantillon de test\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nb_test = 3\n",
    "\n",
    "for nb in nb_test:\n",
    "    sample = test_ds.take(1).as_numpy_iterator()\n",
    "    text, image, labels = next(sample)\n",
    "\n",
    "    plt.imshow(image[0])  # Display the first image in the batch\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Texte : {text[0]}\")\n",
    "\n",
    "    predictions = combined_model.predict(((text, image), image))\n",
    "    top3_pred = multi_label_threshold(predictions, threshold_nb=3)\n",
    "\n",
    "    print(f\"Prédictions : {top3_pred}\")\n",
    "    print(f\"Confiance : {predictions}\")\n",
    "    print(f\"Vrais labels : {labels}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
